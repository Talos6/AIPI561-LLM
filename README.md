# TinyLlama API

A FastAPI-based service for text generation using TinyLlama model with in-memory caching and containerized deployment.

## Features

- ğŸ¤– Local LLM text generation using TinyLlama model
- ğŸ—„ï¸ In-memory caching for improved performance
- ğŸš€ FastAPI with automatic API documentation
- ğŸ³ Docker containerization
- â˜ï¸ AWS App Runner deployment ready

## Quick Start

1. **Install Dependencies**
   ```bash
   pip install -r requirements.txt
   ```

2. **Run the Application**
   ```bash
   python app.py
   ```

## Documentation

For detailed information, please refer to:

- **[API Documentation](docs/api-documentation.md)** - Complete endpoint reference
- **[Technical Overview](docs/technical-overview.md)** - Architecture, components, and design decisions
- **[Deployment Guide](docs/deployment-guide.md)** - Local setup, Docker, and AWS deployment
