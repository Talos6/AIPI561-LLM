# TinyLlama API

A FastAPI-based service for text generation using TinyLlama model with in-memory caching and containerized deployment.

## Features

- 🤖 Local LLM text generation using TinyLlama model
- 🗄️ In-memory caching for improved performance
- 🚀 FastAPI with automatic API documentation
- 🐳 Docker containerization
- ☁️ AWS App Runner deployment ready

## Quick Start

1. **Install Dependencies**
   ```bash
   pip install -r requirements.txt
   ```

2. **Run the Application**
   ```bash
   python app.py
   ```

## Documentation

For detailed information, please refer to:

- **[API Documentation](docs/api-documentation.md)** - Complete endpoint reference
- **[Technical Overview](docs/technical-overview.md)** - Architecture, components, and design decisions
- **[Deployment Guide](docs/deployment-guide.md)** - Local setup, Docker, and AWS deployment
